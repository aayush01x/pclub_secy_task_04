{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import collections\nimport json\nimport pandas as pd\nimport re\nimport string\nimport timeit\nfrom ast import literal_eval\n# !pip install transformers\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\nimport time\nimport torch.nn.utils.prune as prune\nimport numpy as np\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-23T18:21:50.062876Z","iopub.execute_input":"2024-05-23T18:21:50.063790Z","iopub.status.idle":"2024-05-23T18:21:50.073078Z","shell.execute_reply.started":"2024-05-23T18:21:50.063749Z","shell.execute_reply":"2024-05-23T18:21:50.071790Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"PATH_TO_EMBEDS = '/home/aayush/Documents/pclub_secy/task_4/compressed_array.npz'\nPATH_TO_DF = '/home/aayush/Documents/pclub_secy/task_4/compressed_dataframe.csv.gz'","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:21:50.075306Z","iopub.execute_input":"2024-05-23T18:21:50.075977Z","iopub.status.idle":"2024-05-23T18:21:50.085170Z","shell.execute_reply.started":"2024-05-23T18:21:50.075935Z","shell.execute_reply":"2024-05-23T18:21:50.083776Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def run_faiss_search(query_text, top_k):\n    query = [query_text]\n    query_embedding = model.encode(query)\n    scores, index_vals = faiss_index.search(query_embedding, top_k)\n    index_vals_list = index_vals[0]\n    \n    return index_vals_list\n    \n\ndef run_rerank(index_vals_list, query_text):  \n    chunk_list = list(df_data['paragraph'])\n    pred_strings_list = [chunk_list[item] for item in index_vals_list]\n    cross_input_list = []\n    for item in pred_strings_list:\n        new_list = [query_text, item]\n        cross_input_list.append(new_list)\n    df = pd.DataFrame(cross_input_list, columns=['query_text', 'pred_text'])\n    df['original_index'] = index_vals_list\n    cross_scores = cross_encoder.predict(cross_input_list)\n    df['cross_scores'] = cross_scores\n    df_sorted = df.sort_values(by='cross_scores', ascending=False)\n    df_sorted = df_sorted.reset_index(drop=True)\n\n    pred_list = []\n\n    for i in range(0,len(df_sorted)):\n        text = df_sorted.loc[i, 'pred_text']\n        original_index = df_sorted.loc[i, 'original_index']\n        item = {\n            'abstract': text\n        }\n        pred_list.append(item)\n    return pred_list\n\n\ndef print_search_results(pred_list, num_results_to_print):\n    for i in range(0,num_results_to_print):      \n        pred_dict = pred_list[i]\n        para = pred_dict['abstract']\n        print('Abstract:',para)\n        print()\n    \n   \ndef run_search(query_text, num_results_to_print, top_k=5):\n    pred_index_list = run_faiss_search(query_text, top_k)\n    pred_list = run_rerank(pred_index_list, query_text)\n    return pred_list","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:21:50.087312Z","iopub.execute_input":"2024-05-23T18:21:50.088097Z","iopub.status.idle":"2024-05-23T18:21:50.103040Z","shell.execute_reply.started":"2024-05-23T18:21:50.088034Z","shell.execute_reply":"2024-05-23T18:21:50.101692Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\ndevice = 0 if torch.cuda.is_available() else -1\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:21:50.104570Z","iopub.execute_input":"2024-05-23T18:21:50.104948Z","iopub.status.idle":"2024-05-23T18:21:50.120842Z","shell.execute_reply.started":"2024-05-23T18:21:50.104915Z","shell.execute_reply":"2024-05-23T18:21:50.119588Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForQuestionAnswering.from_pretrained(\"navteca/electra-base-squad2\")\ntokenizer = AutoTokenizer.from_pretrained('navteca/electra-base-squad2')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:21:50.123540Z","iopub.execute_input":"2024-05-23T18:21:50.123994Z","iopub.status.idle":"2024-05-23T18:21:55.068541Z","shell.execute_reply.started":"2024-05-23T18:21:50.123950Z","shell.execute_reply":"2024-05-23T18:21:55.066759Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/635 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53fe303dd8124cc8abdc4230280e88a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57674e8a5a3d4447b4ffc861ded4c404"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec1f53b5253643f98100740e3925444b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a04e9db6650c4ad28317cbe5d22ab1fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/134 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfdb3f497f184abe83319818ff998f0a"}},"metadata":{}}]},{"cell_type":"code","source":"nlp = pipeline(\"question-answering\", model = model, tokenizer= tokenizer,device=device)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:21:55.071414Z","iopub.execute_input":"2024-05-23T18:21:55.072024Z","iopub.status.idle":"2024-05-23T18:21:55.089983Z","shell.execute_reply.started":"2024-05-23T18:21:55.071963Z","shell.execute_reply":"2024-05-23T18:21:55.088180Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"modules = [module for module in nlp.model.modules()]\nmods = []\ndef get_modules(mod,parent):\n    mod_list = list(mod._modules.keys())\n    if(len(mod_list)>0):    \n        for mod_n in mod_list:\n#             print(mod_n)\n            if(mod_n in ['qa_outputs']):\n                continue\n            get_modules(mod._modules[mod_n],mod)\n    else:\n        mods.append((str(type(mod)).split('.')[-1][:-2],mod))\n        return","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:21:55.091567Z","iopub.execute_input":"2024-05-23T18:21:55.092017Z","iopub.status.idle":"2024-05-23T18:21:55.104378Z","shell.execute_reply.started":"2024-05-23T18:21:55.091982Z","shell.execute_reply":"2024-05-23T18:21:55.102427Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"get_modules(modules[0],None)\nfinal = [(name,layer) for name,layer in mods if name not in ['Dropout','GELUActivation','LayerNorm','Softmax','Tanh','MatMulWrapper', 'SqueezeBertLayerNorm']]\n\nunc = ['Linear']\nfor name,layer in final:\n    if name not in unc:\n        unc.append(name)\nunc","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:21:55.106292Z","iopub.execute_input":"2024-05-23T18:21:55.106916Z","iopub.status.idle":"2024-05-23T18:21:55.132014Z","shell.execute_reply.started":"2024-05-23T18:21:55.106863Z","shell.execute_reply":"2024-05-23T18:21:55.130215Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['Linear', 'Embedding']"},"metadata":{}}]},{"cell_type":"code","source":"parameters_to_prune = [\n    (module, \"weight\") for name,module in final\n]\nprune.global_unstructured(\n    parameters_to_prune,\n    pruning_method=prune.L1Unstructured, \n    amount=0.3,\n)\nfor j in range(len(final)):\n    if prune.is_pruned(final[j][1]):\n        prune.remove(final[j][1],'weight')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:21:55.133132Z","iopub.execute_input":"2024-05-23T18:21:55.133476Z","iopub.status.idle":"2024-05-23T18:22:10.318373Z","shell.execute_reply.started":"2024-05-23T18:21:55.133446Z","shell.execute_reply":"2024-05-23T18:22:10.317078Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"torch.save(model,'pruned.pt')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:22:10.319850Z","iopub.execute_input":"2024-05-23T18:22:10.320285Z","iopub.status.idle":"2024-05-23T18:22:10.899060Z","shell.execute_reply.started":"2024-05-23T18:22:10.320245Z","shell.execute_reply":"2024-05-23T18:22:10.897961Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nfrom torch.quantization import quantize_dynamic\nimport torch\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"navteca/electra-base-squad2\")\nmodel_q = quantize_dynamic(\n    model=model, qconfig_spec={nn.Linear}, dtype=torch.qint8, inplace=False\n)\n     \nnlp = pipeline(\"question-answering\", model = model_q, tokenizer= \"navteca/electra-base-squad2\")\ntorch.save(model_q,'quantized.pt')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:22:10.904393Z","iopub.execute_input":"2024-05-23T18:22:10.904801Z","iopub.status.idle":"2024-05-23T18:22:13.896816Z","shell.execute_reply.started":"2024-05-23T18:22:10.904767Z","shell.execute_reply":"2024-05-23T18:22:13.895808Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"pip install git+https://github.com/huggingface/optimum-intel.git\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:22:13.898457Z","iopub.execute_input":"2024-05-23T18:22:13.899210Z","iopub.status.idle":"2024-05-23T18:22:50.233127Z","shell.execute_reply.started":"2024-05-23T18:22:13.899168Z","shell.execute_reply":"2024-05-23T18:22:50.231586Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting git+https://github.com/huggingface/optimum-intel.git\n  Cloning https://github.com/huggingface/optimum-intel.git to /tmp/pip-req-build-7bfb70jr\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/optimum-intel.git /tmp/pip-req-build-7bfb70jr\n  Resolved https://github.com/huggingface/optimum-intel.git to commit 1319d7bec80622abdb39b7d0307df6e453e4e903\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum-intel==1.17.0.dev0+1319d7b) (2.1.2+cpu)\nRequirement already satisfied: transformers<4.41.0,>=4.36.0 in /opt/conda/lib/python3.10/site-packages (from optimum-intel==1.17.0.dev0+1319d7b) (4.39.3)\nCollecting optimum~=1.19 (from optimum-intel==1.17.0.dev0+1319d7b)\n  Downloading optimum-1.19.2-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: datasets>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from optimum-intel==1.17.0.dev0+1319d7b) (2.18.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from optimum-intel==1.17.0.dev0+1319d7b) (0.2.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from optimum-intel==1.17.0.dev0+1319d7b) (1.11.4)\nRequirement already satisfied: onnx in /opt/conda/lib/python3.10/site-packages (from optimum-intel==1.17.0.dev0+1319d7b) (1.16.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (6.0.1)\nCollecting coloredlogs (from optimum~=1.19->optimum-intel==1.17.0.dev0+1319d7b)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum~=1.19->optimum-intel==1.17.0.dev0+1319d7b) (1.12)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum-intel==1.17.0.dev0+1319d7b) (4.9.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum-intel==1.17.0.dev0+1319d7b) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum-intel==1.17.0.dev0+1319d7b) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.41.0,>=4.36.0->optimum-intel==1.17.0.dev0+1319d7b) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<4.41.0,>=4.36.0->optimum-intel==1.17.0.dev0+1319d7b) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.41.0,>=4.36.0->optimum-intel==1.17.0.dev0+1319d7b) (0.4.3)\nRequirement already satisfied: protobuf>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from onnx->optimum-intel==1.17.0.dev0+1319d7b) (3.20.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (2024.2.2)\nCollecting humanfriendly>=9.1 (from coloredlogs->optimum~=1.19->optimum-intel==1.17.0.dev0+1319d7b)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum-intel==1.17.0.dev0+1319d7b) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (2023.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum~=1.19->optimum-intel==1.17.0.dev0+1319d7b) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.4.0->optimum-intel==1.17.0.dev0+1319d7b) (1.16.0)\nDownloading optimum-1.19.2-py3-none-any.whl (417 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.0/417.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: optimum-intel\n  Building wheel for optimum-intel (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for optimum-intel: filename=optimum_intel-1.17.0.dev0+1319d7b-py3-none-any.whl size=215281 sha256=2ad5e743004bcde9811bcc09a1304e1892a635eb7463e60f8e07a98125f15c1c\n  Stored in directory: /tmp/pip-ephem-wheel-cache-jlg9x5i6/wheels/99/e8/f9/680e64aac521c117548ed9511e82474364afaaf0637907e987\nSuccessfully built optimum-intel\nInstalling collected packages: humanfriendly, coloredlogs, optimum, optimum-intel\nSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.19.2 optimum-intel-1.17.0.dev0+1319d7b\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!python -m pip install optimum[neural-compressor]\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:22:50.236032Z","iopub.execute_input":"2024-05-23T18:22:50.236523Z","iopub.status.idle":"2024-05-23T18:23:09.138516Z","shell.execute_reply.started":"2024-05-23T18:22:50.236477Z","shell.execute_reply":"2024-05-23T18:23:09.136822Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Requirement already satisfied: optimum[neural-compressor] in /opt/conda/lib/python3.10/site-packages (1.19.2)\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from optimum[neural-compressor]) (15.0.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum[neural-compressor]) (1.12)\nRequirement already satisfied: transformers<4.41.0,>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum[neural-compressor]) (4.39.3)\nRequirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum[neural-compressor]) (2.1.2+cpu)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum[neural-compressor]) (21.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optimum[neural-compressor]) (1.26.4)\nRequirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum[neural-compressor]) (0.22.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum[neural-compressor]) (2.18.0)\nRequirement already satisfied: optimum-intel>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (1.17.0.dev0+1319d7b)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neural-compressor]) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neural-compressor]) (2024.2.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neural-compressor]) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neural-compressor]) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neural-compressor]) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neural-compressor]) (4.9.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from optimum-intel>=1.15.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (0.2.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from optimum-intel>=1.15.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (1.11.4)\nRequirement already satisfied: onnx in /opt/conda/lib/python3.10/site-packages (from optimum-intel>=1.15.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (1.16.0)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[neural-compressor]) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[neural-compressor]) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[neural-compressor]) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[neural-compressor]) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[neural-compressor]) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[neural-compressor]) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[neural-compressor]) (3.9.1)\nCollecting neural-compressor>=2.2.0 (from optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor])\n  Downloading neural_compressor-2.5.1-py3-none-any.whl.metadata (15 kB)\nCollecting onnxruntime<1.15.0 (from optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor])\n  Downloading onnxruntime-1.14.1-cp310-cp310-manylinux_2_27_x86_64.whl.metadata (3.9 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (0.29.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->optimum[neural-compressor]) (3.1.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum[neural-compressor]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum[neural-compressor]) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.41.0,>=4.26.0->transformers[sentencepiece]<4.41.0,>=4.26.0->optimum[neural-compressor]) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<4.41.0,>=4.26.0->transformers[sentencepiece]<4.41.0,>=4.26.0->optimum[neural-compressor]) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.41.0,>=4.26.0->transformers[sentencepiece]<4.41.0,>=4.26.0->optimum[neural-compressor]) (0.4.3)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum[neural-compressor]) (3.20.3)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->optimum[neural-compressor]) (10.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum[neural-compressor]) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neural-compressor]) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neural-compressor]) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neural-compressor]) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neural-compressor]) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neural-compressor]) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neural-compressor]) (4.0.3)\nRequirement already satisfied: deprecated>=1.2.13 in /opt/conda/lib/python3.10/site-packages (from neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (1.2.14)\nRequirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (from neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (4.9.0.80)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (9.5.0)\nRequirement already satisfied: prettytable in /opt/conda/lib/python3.10/site-packages (from neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (3.9.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (9.0.0)\nCollecting schema (from neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor])\n  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (1.2.2)\nCollecting pycocotools (from neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor])\n  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime<1.15.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (23.5.26)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum[neural-compressor]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum[neural-compressor]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum[neural-compressor]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum[neural-compressor]) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum[neural-compressor]) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum[neural-compressor]) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum[neural-compressor]) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum[neural-compressor]) (2023.4)\nRequirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.13->neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (1.14.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum[neural-compressor]) (1.16.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prettytable->neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (0.2.13)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools->neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (3.7.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (3.2.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor>=2.2.0->optimum-intel[neural-compressor]>=1.15.0; extra == \"neural-compressor\"->optimum[neural-compressor]) (1.4.5)\nDownloading neural_compressor-2.5.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.14.1-cp310-cp310-manylinux_2_27_x86_64.whl (5.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\nInstalling collected packages: schema, onnxruntime, pycocotools, neural-compressor\nSuccessfully installed neural-compressor-2.5.1 onnxruntime-1.14.1 pycocotools-2.0.7 schema-0.7.7\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering\nfrom neural_compressor.config import PostTrainingQuantConfig\nfrom optimum.intel.neural_compressor import INCQuantizer\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"navteca/electra-base-squad2\")\n# The directory where the quantized model will be saved\nsave_dir = \"quantized_model\"\n# Load the quantization configuration detailing the quantization we wish to apply\nquantization_config = PostTrainingQuantConfig(approach=\"dynamic\")\nquantizer = INCQuantizer.from_pretrained(model)\n# Apply dynamic quantization and save the resulting model\nquantizer.quantize(quantization_config=quantization_config, save_directory=save_dir)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:23:09.140626Z","iopub.execute_input":"2024-05-23T18:23:09.141154Z","iopub.status.idle":"2024-05-23T18:23:25.684887Z","shell.execute_reply.started":"2024-05-23T18:23:09.141102Z","shell.execute_reply":"2024-05-23T18:23:25.683521Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"2024-05-23 18:23:16 [INFO] Start auto tuning.\n2024-05-23 18:23:16 [INFO] Execute the tuning process due to detect the evaluation function.\n2024-05-23 18:23:16 [INFO] Adaptor has 5 recipes.\n2024-05-23 18:23:16 [INFO] 0 recipes specified by user.\n2024-05-23 18:23:16 [INFO] 3 recipes require future tuning.\n2024-05-23 18:23:16 [INFO] *** Initialize auto tuning\n2024-05-23 18:23:16 [INFO] {\n2024-05-23 18:23:16 [INFO]     'PostTrainingQuantConfig': {\n2024-05-23 18:23:16 [INFO]         'AccuracyCriterion': {\n2024-05-23 18:23:16 [INFO]             'criterion': 'relative',\n2024-05-23 18:23:16 [INFO]             'higher_is_better': True,\n2024-05-23 18:23:16 [INFO]             'tolerable_loss': 0.01,\n2024-05-23 18:23:16 [INFO]             'absolute': None,\n2024-05-23 18:23:16 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7d67bb7170d0>>,\n2024-05-23 18:23:16 [INFO]             'relative': 0.01\n2024-05-23 18:23:16 [INFO]         },\n2024-05-23 18:23:16 [INFO]         'approach': 'post_training_dynamic_quant',\n2024-05-23 18:23:16 [INFO]         'backend': 'default',\n2024-05-23 18:23:16 [INFO]         'calibration_sampling_size': [\n2024-05-23 18:23:16 [INFO]             100\n2024-05-23 18:23:16 [INFO]         ],\n2024-05-23 18:23:16 [INFO]         'device': 'cpu',\n2024-05-23 18:23:16 [INFO]         'diagnosis': False,\n2024-05-23 18:23:16 [INFO]         'domain': 'auto',\n2024-05-23 18:23:16 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n2024-05-23 18:23:16 [INFO]         'excluded_precisions': [\n2024-05-23 18:23:16 [INFO]         ],\n2024-05-23 18:23:16 [INFO]         'framework': 'pytorch_fx',\n2024-05-23 18:23:16 [INFO]         'inputs': [\n2024-05-23 18:23:16 [INFO]         ],\n2024-05-23 18:23:16 [INFO]         'model_name': '',\n2024-05-23 18:23:16 [INFO]         'ni_workload_name': 'quantization',\n2024-05-23 18:23:16 [INFO]         'op_name_dict': None,\n2024-05-23 18:23:16 [INFO]         'op_type_dict': None,\n2024-05-23 18:23:16 [INFO]         'outputs': [\n2024-05-23 18:23:16 [INFO]         ],\n2024-05-23 18:23:16 [INFO]         'quant_format': 'default',\n2024-05-23 18:23:16 [INFO]         'quant_level': 'auto',\n2024-05-23 18:23:16 [INFO]         'recipes': {\n2024-05-23 18:23:16 [INFO]             'smooth_quant': False,\n2024-05-23 18:23:16 [INFO]             'smooth_quant_args': {\n2024-05-23 18:23:16 [INFO]             },\n2024-05-23 18:23:16 [INFO]             'layer_wise_quant': False,\n2024-05-23 18:23:16 [INFO]             'layer_wise_quant_args': {\n2024-05-23 18:23:16 [INFO]             },\n2024-05-23 18:23:16 [INFO]             'fast_bias_correction': False,\n2024-05-23 18:23:16 [INFO]             'weight_correction': False,\n2024-05-23 18:23:16 [INFO]             'gemm_to_matmul': True,\n2024-05-23 18:23:16 [INFO]             'graph_optimization_level': None,\n2024-05-23 18:23:16 [INFO]             'first_conv_or_matmul_quantization': True,\n2024-05-23 18:23:16 [INFO]             'last_conv_or_matmul_quantization': True,\n2024-05-23 18:23:16 [INFO]             'pre_post_process_quantization': True,\n2024-05-23 18:23:16 [INFO]             'add_qdq_pair_to_weight': False,\n2024-05-23 18:23:16 [INFO]             'optypes_to_exclude_output_quant': [\n2024-05-23 18:23:16 [INFO]             ],\n2024-05-23 18:23:16 [INFO]             'dedicated_qdq_pair': False,\n2024-05-23 18:23:16 [INFO]             'rtn_args': {\n2024-05-23 18:23:16 [INFO]             },\n2024-05-23 18:23:16 [INFO]             'awq_args': {\n2024-05-23 18:23:16 [INFO]             },\n2024-05-23 18:23:16 [INFO]             'gptq_args': {\n2024-05-23 18:23:16 [INFO]             },\n2024-05-23 18:23:16 [INFO]             'teq_args': {\n2024-05-23 18:23:16 [INFO]             },\n2024-05-23 18:23:16 [INFO]             'autoround_args': {\n2024-05-23 18:23:16 [INFO]             }\n2024-05-23 18:23:16 [INFO]         },\n2024-05-23 18:23:16 [INFO]         'reduce_range': None,\n2024-05-23 18:23:16 [INFO]         'TuningCriterion': {\n2024-05-23 18:23:16 [INFO]             'max_trials': 100,\n2024-05-23 18:23:16 [INFO]             'objective': [\n2024-05-23 18:23:16 [INFO]                 'performance'\n2024-05-23 18:23:16 [INFO]             ],\n2024-05-23 18:23:16 [INFO]             'strategy': 'basic',\n2024-05-23 18:23:16 [INFO]             'strategy_kwargs': None,\n2024-05-23 18:23:16 [INFO]             'timeout': 0\n2024-05-23 18:23:16 [INFO]         },\n2024-05-23 18:23:16 [INFO]         'use_bf16': True\n2024-05-23 18:23:16 [INFO]     }\n2024-05-23 18:23:16 [INFO] }\n2024-05-23 18:23:16 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n/opt/conda/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n  torch.has_cuda,\n/opt/conda/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n  torch.has_cudnn,\n/opt/conda/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n  torch.has_mps,\n/opt/conda/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n  torch.has_mkldnn,\n2024-05-23 18:23:18 [INFO]  Found 12 blocks\n2024-05-23 18:23:18 [INFO] Attention Blocks: 12\n2024-05-23 18:23:18 [INFO] FFN Blocks: 12\n2024-05-23 18:23:18 [INFO] Pass query framework capability elapsed time: 1051.14 ms\n2024-05-23 18:23:18 [INFO] Get FP32 model baseline.\n2024-05-23 18:23:18 [INFO] Save tuning history to /kaggle/working/nc_workspace/2024-05-23_18-23-09/./history.snapshot.\n2024-05-23 18:23:18 [INFO] FP32 baseline is: [Accuracy: 1.0000, Duration (seconds): 0.0000]\n2024-05-23 18:23:18 [INFO] Quantize the model with default config.\n2024-05-23 18:23:18 [INFO] Fx trace of the entire model failed, We will conduct auto quantization\n2024-05-23 18:23:24 [INFO] |******Mixed Precision Statistics******|\n2024-05-23 18:23:24 [INFO] +-----------------+----------+---------+\n2024-05-23 18:23:24 [INFO] |     Op Type     |  Total   |   INT8  |\n2024-05-23 18:23:24 [INFO] +-----------------+----------+---------+\n2024-05-23 18:23:24 [INFO] |    Embedding    |    3     |    3    |\n2024-05-23 18:23:24 [INFO] |      Linear     |    73    |    73   |\n2024-05-23 18:23:24 [INFO] +-----------------+----------+---------+\n2024-05-23 18:23:24 [INFO] Pass quantize model elapsed time: 6757.43 ms\n2024-05-23 18:23:24 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 1.0000|1.0000, Duration (seconds) (int8|fp32): 0.0000|0.0000], Best tune result is: [Accuracy: 1.0000, Duration (seconds): 0.0000]\n2024-05-23 18:23:24 [INFO] |**********************Tune Result Statistics**********************|\n2024-05-23 18:23:24 [INFO] +--------------------+----------+---------------+------------------+\n2024-05-23 18:23:24 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |\n2024-05-23 18:23:24 [INFO] +--------------------+----------+---------------+------------------+\n2024-05-23 18:23:24 [INFO] |      Accuracy      | 1.0000   |    1.0000     |     1.0000       |\n2024-05-23 18:23:24 [INFO] | Duration (seconds) | 0.0000   |    0.0000     |     0.0000       |\n2024-05-23 18:23:24 [INFO] +--------------------+----------+---------------+------------------+\n2024-05-23 18:23:24 [INFO] [Strategy] Found a model that meets the accuracy requirements.\n2024-05-23 18:23:24 [INFO] Save tuning history to /kaggle/working/nc_workspace/2024-05-23_18-23-09/./history.snapshot.\n2024-05-23 18:23:24 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.\n2024-05-23 18:23:24 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n2024-05-23 18:23:24 [INFO] Save deploy yaml to /kaggle/working/nc_workspace/2024-05-23_18-23-09/deploy.yaml\nModel weights saved to quantized_model/pytorch_model.bin\nConfiguration saved in quantized_model/inc_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"def pred_theme_ans(questions, pred_out,threshold=0.05):\n  theme = questions[0][\"Theme\"]\n  for question in questions:\n    ans = {}\n    ans[\"question_id\"] = question[\"id\"]\n    QA_input = {\n      'question':question['Question'],\n      'context': question['Paragraph']\n      }\n    res = nlp(QA_input) \n    score = res['score']\n\n    if (score<threshold):\n       ans[\"answers\"]= ''\n    else:\n      ans[\"answers\"]=res['answer']\n    pred_out.append(ans)\n\ndef normalize_answer(s):\n  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n  def remove_articles(text):\n    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n    return re.sub(regex, ' ', text)\n  def white_space_fix(text):\n    return ' '.join(text.split())\n  def remove_punc(text):\n    exclude = set(string.punctuation)\n    return ''.join(ch for ch in text if ch not in exclude)\n  def lower(text):\n    return text.lower()\n  return white_space_fix(remove_articles(remove_punc(lower(str(s)))))\ndef get_tokens(s):\n  if not s: return []\n  return normalize_answer(s).split()\ndef calc_f1(a_gold, a_pred):\n  gold_toks = get_tokens(a_gold)\n  pred_toks = get_tokens(a_pred)\n  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n  num_same = sum(common.values())\n  if len(gold_toks) == 0 or len(pred_toks) == 0:\n    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n    return int(gold_toks == pred_toks)\n  if num_same == 0:\n    return 0\n  precision = 1.0 * num_same / len(pred_toks)\n  recall = 1.0 * num_same / len(gold_toks)\n  f1 = (2 * precision * recall) / (precision + recall)\n  return f1\n\n# Calculate the f1 score between prediction and multiple ground truths\ndef calc_max_f1(predicted, ground_truths):\n  max_f1 = 0\n  for ground_truth in ground_truths:\n    f1 = calc_f1(predicted, ground_truth)\n    max_f1 = max(max_f1, f1)\n  try:\n    ground_truths[0]\n  except Exception as e:\n    if predicted!=predicted:\n      max_f1 = 1\n  return max_f1","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:23:25.686901Z","iopub.execute_input":"2024-05-23T18:23:25.687822Z","iopub.status.idle":"2024-05-23T18:23:25.706707Z","shell.execute_reply.started":"2024-05-23T18:23:25.687776Z","shell.execute_reply":"2024-05-23T18:23:25.705269Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"navteca/electra-base-squad2\")\n\nfrom optimum.intel.neural_compressor import INCModelForQuestionAnswering\n\nloaded_model_from_hub = INCModelForQuestionAnswering.from_pretrained(\n    \"quantized_model\"\n)\n\nnlp = pipeline(\"question-answering\", model = loaded_model_from_hub, tokenizer= tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:23:25.708476Z","iopub.execute_input":"2024-05-23T18:23:25.709557Z","iopub.status.idle":"2024-05-23T18:23:36.442111Z","shell.execute_reply.started":"2024-05-23T18:23:25.709509Z","shell.execute_reply":"2024-05-23T18:23:36.440961Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"loading configuration file quantized_model/inc_config.json\nINCConfig {\n  \"distillation\": {},\n  \"neural_compressor_version\": \"2.5.1\",\n  \"optimum_version\": \"1.19.2\",\n  \"pruning\": {},\n  \"quantization\": {\n    \"dataset_num_samples\": null,\n    \"is_static\": false\n  },\n  \"save_onnx_model\": false,\n  \"torch_version\": \"2.1.2+cpu\",\n  \"transformers_version\": \"4.39.3\"\n}\n\nQuantized model was obtained with torch version 2.1.2+cpu but 2.1.2+cpu was found.\n","output_type":"stream"}]},{"cell_type":"code","source":"# from tqdm import tqdm\n# # questions = json.loads(questions_df.to_json(orient=\"records\"))\n# # theme_intervals = json.loads(theme_df.to_json(orient=\"records\"))\n# pred_out = []\n# theme_inf_time = {}\n# execution_times = []\n# for theme_interval in tqdm(theme_intervals):\n#   theme_ques = questions[int(theme_interval[\"start\"]) - 1: int(theme_interval[\"end\"])]\n#   execution_time = timeit.timeit(lambda: pred_theme_ans(theme_ques, pred_out), number=1)\n#   execution_times.append(execution_time/len(theme_ques))\n#   theme_inf_time[theme_interval[\"theme\"]] = execution_time * 1000 # in milliseconds.\n# pred_df = pd.DataFrame.from_records(pred_out)\n# # Write prediction to a CSV file. Teams are required to submit this csv file.\n# pred_df.to_csv('/content/output_prediction.csv', index=False)\n# print(\"\\navg_inference_time:\",round(sum(execution_times)/len(theme_intervals),3)*1000)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:23:36.443748Z","iopub.execute_input":"2024-05-23T18:23:36.444208Z","iopub.status.idle":"2024-05-23T18:23:36.451161Z","shell.execute_reply.started":"2024-05-23T18:23:36.444167Z","shell.execute_reply":"2024-05-23T18:23:36.450231Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def pred_theme_ans_sing(question, para,threshold=0.001):\n    \n#   theme = questions[0][\"Theme\"]\n#   for question in questions:\n#     ans = {}\n#     # Get Question Id\n#     ans[\"question_id\"] = question[\"id\"]\n    QA_input = {\n      'question':question,\n      'context': para\n      }\n    \n    # Get Result, Contains, Answer Start, Answer End, Answer and the Confidence Score\n    res = nlp(QA_input) \n    score = res['score']\n    print(score)\n    # If Confidence Score is less than the threshold then dont Answer,otherwise answer\n    if (score<threshold):\n       ans= ''\n    else:\n      ans=res['answer']\n    print(ans)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:23:36.452588Z","iopub.execute_input":"2024-05-23T18:23:36.453223Z","iopub.status.idle":"2024-05-23T18:23:36.517809Z","shell.execute_reply.started":"2024-05-23T18:23:36.453190Z","shell.execute_reply":"2024-05-23T18:23:36.516555Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"q = \"What were the names given to the various dialects predating the 19th century?\"\npara = \"Throughout the history of the South Slavs, the vernacular, literary, and written languages (e.g. Chakavian, Kajkavian, Shtokavian) of the various regions and ethnicities developed and diverged independently. Prior to the 19th century, they were collectively called \\\"Illyric\\\", \\\"Slavic\\\", \\\"Slavonian\\\", \\\"Bosnian\\\", \\\"Dalmatian\\\", \\\"Serbian\\\" or \\\"Croatian\\\". As such, the term Serbo-Croatian was first used by Jacob Grimm in 1824, popularized by the Vienna philologist Jernej Kopitar in the following decades, and accepted by Croatian Zagreb grammarians in 1854 and 1859. At that time, Serb and Croat lands were still part of the Ottoman and Austrian Empires. Officially, the language was called variously Serbo-Croat, Croato-Serbian, Serbian and Croatian, Croatian and Serbian, Serbian or Croatian, Croatian or Serbian. Unofficially, Serbs and Croats typically called the language \\\"Serbian\\\" or \\\"Croatian\\\", respectively, without implying a distinction between the two, and again in independent Bosnia and Herzegovina, \\\"Bosnian\\\", \\\"Croatian\\\", and \\\"Serbian\\\" were considered to be three names of a single official language. Croatian linguist Dalibor Brozović advocated the term Serbo-Croatian as late as 1988, claiming that in an analogy with Indo-European, Serbo-Croatian does not only name the two components of the same language, but simply charts the limits of the region in which it is spoken and includes everything between the limits (‘Bosnian’ and ‘Montenegrin’). Today, use of the term \\\"Serbo-Croatian\\\" is controversial due to the prejudice that nation and language must match. It is still used for lack of a succinct alternative, though alternative names have been used, such as Bosnian/Croatian/Serbian (BCS), which is often seen in political contexts such as the Hague War Crimes tribunal.\"","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:23:36.519353Z","iopub.execute_input":"2024-05-23T18:23:36.520477Z","iopub.status.idle":"2024-05-23T18:23:36.534235Z","shell.execute_reply.started":"2024-05-23T18:23:36.520443Z","shell.execute_reply":"2024-05-23T18:23:36.532817Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"pred_theme_ans_sing(q,para)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:23:36.535943Z","iopub.execute_input":"2024-05-23T18:23:36.536302Z","iopub.status.idle":"2024-05-23T18:23:37.415630Z","shell.execute_reply.started":"2024-05-23T18:23:36.536272Z","shell.execute_reply":"2024-05-23T18:23:37.413366Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"0.006296506151556969\n\"Serbian\" or \"Croatian\"\n","output_type":"stream"}]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Sentences are encoded by calling model.encode()\nembeddings = model.encode(chunk_list)\n\nprint(embeddings.shape)\nprint('Embedding length', embeddings.shape[1])\nress = run_search(str(q), 5)[0]['abstract']\npred_theme_ans_sing(q,ress)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}